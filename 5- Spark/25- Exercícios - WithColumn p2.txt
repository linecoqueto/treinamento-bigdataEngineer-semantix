-----------------------------------------------
1. Criar um dataframe para ler o arquivo no HDFS /user/<nome/data/juros_selic/juros_selic
-----------------------------------------------

from pyspark.sql.functions import *
from pyspark.sql.types import *

juros_selec = spark.read.csv("/user/aline/data/juros_selic/juros_selic", sep=";", header="true")

juros_ano = juros_selic.withColumn("ano", split(col("data"),"/").getItem(2))

juros_valor = juros_valor.withColumn("valor", regexp_replace(col("valor"),"\,","\.").cast(FloatType()))

-----------------------------------------------
2. Agrupar todas as datas pelo ano em ordem decrescente e salvar a quantidade de meses ocorridos, 
o valor médio, mínimo e máximo do campo valor com a seguinte estrutura:
-----------------------------------------------

juros_relatorio = juros_valor.groupBy("ano").agg(count("ano").alias("Meses"),
	format_number(avg("valor"),2).alias("Valor Medio"),
	min("valor").alias("Valor minimo"),
	max("valor").alias("Valor maximo").sort(desc("ano")).withColumnRenamed("ano", "Anual")

juros_relatorio.printSchema()
juros_relatorio.show()

-----------------------------------------------
3. Salvar no hdfs:///user/<nome>/relatorioAnual com compressão zlib e formato orc
-----------------------------------------------

juros_relatorio.write.orc("/user/aline/relatorioAnual", compression="zlib")

!hdfs dfs -ls /user/aline/relatorioAnual